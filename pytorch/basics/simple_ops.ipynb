{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basic Functions Overview\n",
    "\n",
    "This notebook provides a comprehensive guide to PyTorch's main functionality, their purpose, and practical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.0.2\n",
      "PyTorch version: 2.8.0\n",
      "MPS device found!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "# No CUDA available on mac, use MPS (Metal Performance Shaders) if available (for Apple Silicon)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Python list:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "From NumPy array:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Zeros like x:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create from Python lists and NumPy arrays\n",
    "from_list = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"From Python list:\")\n",
    "print(from_list)\n",
    "\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "from_numpy = torch.tensor(numpy_array)\n",
    "print(\"\\nFrom NumPy array:\")\n",
    "print(from_numpy)\n",
    "\n",
    "# Create like another tensor (same shape)\n",
    "x = torch.rand(2, 3)\n",
    "zeros_like = torch.zeros_like(x)\n",
    "ones_like = torch.ones_like(x)\n",
    "print(\"\\nZeros like x:\")\n",
    "print(zeros_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arange tensor (0 to 10, step 2):\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "Linspace tensor (5 points from 0 to 10):\n",
      "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n",
      "\n",
      "Identity matrix (3x3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "Full tensor (filled with 7.5):\n",
      "tensor([[7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with specific ranges\n",
    "arange_tensor = torch.arange(0, 10, 2)  # start, end, step\n",
    "print(\"Arange tensor (0 to 10, step 2):\")\n",
    "print(arange_tensor)\n",
    "\n",
    "# Linear spacing\n",
    "linspace_tensor = torch.linspace(0, 10, 5)  # start, end, number of points\n",
    "print(\"\\nLinspace tensor (5 points from 0 to 10):\")\n",
    "print(linspace_tensor)\n",
    "\n",
    "# Eye matrix (identity matrix)\n",
    "eye_tensor = torch.eye(3)\n",
    "print(\"\\nIdentity matrix (3x3):\")\n",
    "print(eye_tensor)\n",
    "\n",
    "# Full tensor (filled with a specific value)\n",
    "full_tensor = torch.full((2, 3), 7.5)\n",
    "print(\"\\nFull tensor (filled with 7.5):\")\n",
    "print(full_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Zeros tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Ones tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Random tensor (uniform):\n",
      "tensor([[0.9932, 0.3909, 0.9066],\n",
      "        [0.7459, 0.9807, 0.1446]])\n",
      "\n",
      "Random tensor (normal):\n",
      "tensor([[-0.5731,  1.5105, -0.0537],\n",
      "        [-0.8645, -1.1914,  0.2320]])\n"
     ]
    }
   ],
   "source": [
    "# Empty tensor (uninitialized values)\n",
    "empty_tensor = torch.empty(2, 3)\n",
    "print(\"Empty tensor:\")\n",
    "print(empty_tensor)\n",
    "\n",
    "# Zeros tensor\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "print(\"\\nZeros tensor:\")\n",
    "print(zeros_tensor)\n",
    "\n",
    "# Ones tensor\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(\"\\nOnes tensor:\")\n",
    "print(ones_tensor)\n",
    "\n",
    "# Random tensor (uniform distribution [0, 1))\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "print(\"\\nRandom tensor (uniform):\")\n",
    "print(rand_tensor)\n",
    "\n",
    "# Random tensor (normal distribution, mean=0, std=1)\n",
    "randn_tensor = torch.randn(2, 3)\n",
    "print(\"\\nRandom tensor (normal):\")\n",
    "print(randn_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensor Creation Operations\n",
    "\n",
    "**Purpose**: Create tensors from scratch with various initialization patterns.\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1463, 0.8709, 0.4684],\n",
      "        [0.1728, 0.9435, 0.2396],\n",
      "        [0.6396, 0.8025, 0.0146],\n",
      "        [0.4222, 0.5271, 0.3238],\n",
      "        [0.8552, 0.3055, 0.6075]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4])\n",
      "Size: torch.Size([3, 4])\n",
      "Number of dimensions: 2\n",
      "Total elements: 12\n",
      "\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Requires gradient: False\n",
      "Is contiguous: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "\n",
    "# Shape and size\n",
    "print(f\"Shape: {x.shape}\")  # or x.size()\n",
    "print(f\"Size: {x.size()}\")\n",
    "print(f\"Number of dimensions: {x.ndim}\")\n",
    "print(f\"Total elements: {x.numel()}\")\n",
    "\n",
    "# Data type\n",
    "print(f\"\\nData type: {x.dtype}\")\n",
    "\n",
    "# Device (CPU or GPU)\n",
    "print(f\"Device: {x.device}\")\n",
    "\n",
    "# Check if requires gradient\n",
    "print(f\"Requires gradient: {x.requires_grad}\")\n",
    "\n",
    "# Memory layout\n",
    "print(f\"Is contiguous: {x.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all elements: tensor(21.)\n",
      "Sum along rows (dim=0): tensor([5., 7., 9.])\n",
      "Sum along columns (dim=1): tensor([ 6., 15.])\n",
      "\n",
      "Mean: tensor(3.5000)\n",
      "Mean along dim=0: tensor([2.5000, 3.5000, 4.5000])\n",
      "\n",
      "Max: tensor(6.)\n",
      "Max along dim=1: torch.return_types.max(\n",
      "values=tensor([3., 6.]),\n",
      "indices=tensor([2, 2]))\n",
      "\n",
      "Min: tensor(1.)\n",
      "Argmax (index of max): tensor(5)\n",
      "Argmin (index of min): tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Reduction operations (reduce tensor to scalar or along dimension)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"Sum of all elements:\", torch.sum(x))\n",
    "print(\"Sum along rows (dim=0):\", torch.sum(x, dim=0))\n",
    "print(\"Sum along columns (dim=1):\", torch.sum(x, dim=1))\n",
    "\n",
    "print(\"\\nMean:\", torch.mean(x))\n",
    "print(\"Mean along dim=0:\", torch.mean(x, dim=0))\n",
    "\n",
    "print(\"\\nMax:\", torch.max(x))\n",
    "print(\"Max along dim=1:\", torch.max(x, dim=1))  # returns (values, indices)\n",
    "\n",
    "print(\"\\nMin:\", torch.min(x))\n",
    "print(\"Argmax (index of max):\", torch.argmax(x))\n",
    "print(\"Argmin (index of min):\", torch.argmin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential: tensor([ 2.7183,  7.3891, 20.0855, 54.5982])\n",
      "Natural log: tensor([0.0000, 0.6931, 1.0986, 1.3863])\n",
      "Square root: tensor([1.0000, 1.4142, 1.7321, 2.0000])\n",
      "Absolute value: tensor([1, 2, 3])\n",
      "Sine: tensor([ 0.8415,  0.9093,  0.1411, -0.7568])\n",
      "Cosine: tensor([ 0.5403, -0.4161, -0.9900, -0.6536])\n",
      "\n",
      "Round: tensor([1., 3., 4.])\n",
      "Floor: tensor([1., 2., 3.])\n",
      "Ceil: tensor([2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Mathematical functions\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(\"Exponential:\", torch.exp(x))\n",
    "print(\"Natural log:\", torch.log(x))\n",
    "print(\"Square root:\", torch.sqrt(x))\n",
    "print(\"Absolute value:\", torch.abs(torch.tensor([-1, -2, 3])))\n",
    "print(\"Sine:\", torch.sin(x))\n",
    "print(\"Cosine:\", torch.cos(x))\n",
    "\n",
    "# Rounding\n",
    "y = torch.tensor([1.4, 2.6, 3.5])\n",
    "print(\"\\nRound:\", torch.round(y))\n",
    "print(\"Floor:\", torch.floor(y))\n",
    "print(\"Ceil:\", torch.ceil(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([1., 2., 3.])\n",
      "After x.add_(5): tensor([6., 7., 8.])\n",
      "After x.mul_(2): tensor([12., 14., 16.])\n"
     ]
    }
   ],
   "source": [
    "# In-place operations (modify the original tensor, indicated by _ suffix)\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original x:\", x)\n",
    "\n",
    "x.add_(5)  # x = x + 5\n",
    "print(\"After x.add_(5):\", x)\n",
    "\n",
    "x.mul_(2)  # x = x * 2\n",
    "print(\"After x.mul_(2):\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      "tensor([5., 7., 9.])\n",
      "tensor([5., 7., 9.])\n",
      "\n",
      "Subtraction:\n",
      "tensor([-3., -3., -3.])\n",
      "tensor([-3., -3., -3.])\n",
      "\n",
      "Multiplication:\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([ 4., 10., 18.])\n",
      "\n",
      "Division:\n",
      "tensor([0.2500, 0.4000, 0.5000])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n",
      "\n",
      "Power:\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise arithmetic operations\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Addition\n",
    "print(\"Addition:\")\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# Subtraction\n",
    "print(\"\\nSubtraction:\")\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "\n",
    "# Multiplication\n",
    "print(\"\\nMultiplication:\")\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "\n",
    "# Division\n",
    "print(\"\\nDivision:\")\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "\n",
    "# Power\n",
    "print(\"\\nPower:\")\n",
    "print(x ** 2)\n",
    "print(torch.pow(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor Operations (Arithmetic & Mathematical)\n",
    "\n",
    "**Purpose**: Perform element-wise and reduction operations on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "After x[0, 0] = 100:\n",
      "tensor([[100,   2,   3],\n",
      "        [  4,   5,   6],\n",
      "        [  7,   8,   9]])\n",
      "\n",
      "After setting second column to 0:\n",
      "tensor([[100,   0,   3],\n",
      "        [  4,   0,   6],\n",
      "        [  7,   0,   9]])\n",
      "\n",
      "After clamping values > 50 to 50:\n",
      "tensor([[50,  0,  3],\n",
      "        [ 4,  0,  6],\n",
      "        [ 7,  0,  9]])\n"
     ]
    }
   ],
   "source": [
    "# Modifying values with indexing\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "print(\"Original:\")\n",
    "print(x)\n",
    "\n",
    "# Modify single element\n",
    "x[0, 0] = 100\n",
    "print(\"\\nAfter x[0, 0] = 100:\")\n",
    "print(x)\n",
    "\n",
    "# Modify slice\n",
    "x[:, 1] = 0\n",
    "print(\"\\nAfter setting second column to 0:\")\n",
    "print(x)\n",
    "\n",
    "# Modify with boolean mask\n",
    "x[x > 50] = 50\n",
    "print(\"\\nAfter clamping values > 50 to 50:\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask (x > 5):\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [ True,  True,  True]])\n",
      "\n",
      "Elements > 5: tensor([6, 7, 8, 9])\n",
      "\n",
      "Rows 0 and 2:\n",
      "tensor([[1, 2, 3],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "Replace elements <= 5 with 0:\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Advanced indexing\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "# Boolean indexing\n",
    "mask = x > 5\n",
    "print(\"Mask (x > 5):\")\n",
    "print(mask)\n",
    "print(\"\\nElements > 5:\", x[mask])\n",
    "\n",
    "# Index with tensor\n",
    "indices = torch.tensor([0, 2])\n",
    "print(\"\\nRows 0 and 2:\")\n",
    "print(x[indices])\n",
    "\n",
    "# torch.where (conditional selection)\n",
    "result = torch.where(x > 5, x, torch.tensor(0))\n",
    "print(\"\\nReplace elements <= 5 with 0:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n",
      "Element at [0, 1]: tensor(2)\n",
      "\n",
      "First row: tensor([1, 2, 3, 4])\n",
      "Last row: tensor([ 9, 10, 11, 12])\n",
      "\n",
      "First column: tensor([1, 5, 9])\n",
      "Last column: tensor([ 4,  8, 12])\n",
      "\n",
      "First 2 rows: tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "\n",
      "First 2 rows, first 3 columns:\n",
      "tensor([[1, 2, 3],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Basic indexing (similar to NumPy)\n",
    "x = torch.tensor([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "\n",
    "# Single element\n",
    "print(\"\\nElement at [0, 1]:\", x[0, 1])\n",
    "\n",
    "# Single row\n",
    "print(\"\\nFirst row:\", x[0])\n",
    "print(\"Last row:\", x[-1])\n",
    "\n",
    "# Single column\n",
    "print(\"\\nFirst column:\", x[:, 0])\n",
    "print(\"Last column:\", x[:, -1])\n",
    "\n",
    "# Slice ranges\n",
    "print(\"\\nFirst 2 rows:\", x[:2])\n",
    "print(\"\\nFirst 2 rows, first 3 columns:\")\n",
    "print(x[:2, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Indexing and Slicing\n",
    "\n",
    "**Purpose**: Access and modify specific elements or subsets of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "Chunk into 2 parts along dim=0:\n",
      "Chunk 0:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Chunk 1:\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "Split into sizes [1, 3] along dim=0:\n",
      "Split 0 (shape torch.Size([1, 3])):\n",
      "tensor([[0, 1, 2]])\n",
      "Split 1 (shape torch.Size([3, 3])):\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# Split and chunk tensors\n",
    "x = torch.arange(12).reshape(4, 3)\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "\n",
    "# Split into equal chunks\n",
    "chunks = torch.chunk(x, 2, dim=0)  # Split into 2 chunks along dim=0\n",
    "print(\"\\nChunk into 2 parts along dim=0:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "\n",
    "# Split with specific sizes\n",
    "splits = torch.split(x, [1, 3], dim=0)  # Split into sizes [1, 3]\n",
    "print(\"\\nSplit into sizes [1, 3] along dim=0:\")\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Split {i} (shape {split.shape}):\")\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "y:\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "\n",
      "Concatenate along dim=0 (vertical stack):\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "\n",
      "Concatenate along dim=1 (horizontal stack):\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "\n",
      "Stack along dim=0 (shape: torch.Size([2, 2, 2]) ):\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate and stack tensors\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"x:\")\n",
    "print(x)\n",
    "print(\"\\ny:\")\n",
    "print(y)\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "concat_0 = torch.cat([x, y], dim=0)\n",
    "print(\"\\nConcatenate along dim=0 (vertical stack):\")\n",
    "print(concat_0)\n",
    "\n",
    "# Concatenate along dimension 1 (columns)\n",
    "concat_1 = torch.cat([x, y], dim=1)\n",
    "print(\"\\nConcatenate along dim=1 (horizontal stack):\")\n",
    "print(concat_1)\n",
    "\n",
    "# Stack creates a new dimension\n",
    "stacked = torch.stack([x, y], dim=0)\n",
    "print(\"\\nStack along dim=0 (shape:\", stacked.shape, \"):\")\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (2x3):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Transposed (3x2):\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "\n",
      "Original shape: torch.Size([2, 3, 4])\n",
      "After permute(2, 0, 1): torch.Size([4, 2, 3])\n",
      "\n",
      "\n",
      "Original:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Flattened: tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Transpose and permute\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(\"Original (2x3):\")\n",
    "print(x)\n",
    "\n",
    "# Transpose (swap 2 dimensions)\n",
    "transposed = x.transpose(0, 1)  # or x.T\n",
    "print(\"\\nTransposed (3x2):\")\n",
    "print(transposed)\n",
    "\n",
    "# Permute (rearrange all dimensions)\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(\"\\n\\nOriginal shape:\", x.shape)\n",
    "\n",
    "permuted = x.permute(2, 0, 1)  # Reorder to (4, 2, 3)\n",
    "print(\"After permute(2, 0, 1):\", permuted.shape)\n",
    "\n",
    "# Flatten\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(\"\\n\\nOriginal:\")\n",
    "print(x)\n",
    "\n",
    "flattened = x.flatten()\n",
    "print(\"\\nFlattened:\", flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze and Unsqueeze (add/remove dimensions of size 1)\n",
    "x = torch.tensor([[[1], [2], [3]]])\n",
    "print(\"Original shape:\", x.shape)  # torch.Size([1, 3, 1])\n",
    "print(x)\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "squeezed = x.squeeze()\n",
    "print(\"\\nAfter squeeze():\", squeezed.shape)\n",
    "print(squeezed)\n",
    "\n",
    "# Remove specific dimension\n",
    "squeezed_dim = x.squeeze(0)  # Remove dimension 0\n",
    "print(\"\\nAfter squeeze(0):\", squeezed_dim.shape)\n",
    "\n",
    "# Add dimension\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(\"\\n\\nOriginal shape:\", x.shape)\n",
    "\n",
    "unsqueezed = x.unsqueeze(0)  # Add dimension at position 0\n",
    "print(\"After unsqueeze(0):\", unsqueezed.shape)\n",
    "print(unsqueezed)\n",
    "\n",
    "unsqueezed2 = x.unsqueeze(1)  # Add dimension at position 1\n",
    "print(\"\\nAfter unsqueeze(1):\", unsqueezed2.shape)\n",
    "print(unsqueezed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape tensors\n",
    "x = torch.arange(12)\n",
    "print(\"Original tensor (shape\", x.shape, \"):\")\n",
    "print(x)\n",
    "\n",
    "# Reshape to 3x4\n",
    "reshaped = x.reshape(3, 4)\n",
    "print(\"\\nReshaped to (3, 4):\")\n",
    "print(reshaped)\n",
    "\n",
    "# Reshape to 2x6\n",
    "reshaped2 = x.reshape(2, 6)\n",
    "print(\"\\nReshaped to (2, 6):\")\n",
    "print(reshaped2)\n",
    "\n",
    "# Use -1 to infer dimension\n",
    "reshaped3 = x.reshape(3, -1)  # -1 will be computed as 4\n",
    "print(\"\\nReshaped to (3, -1) -> (3, 4):\")\n",
    "print(reshaped3)\n",
    "\n",
    "# View (similar to reshape, but requires contiguous tensor)\n",
    "viewed = x.view(4, 3)\n",
    "print(\"\\nView as (4, 3):\")\n",
    "print(viewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reshaping and Transformations\n",
    "\n",
    "**Purpose**: Change tensor dimensions and structure without modifying the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix decompositions\n",
    "A = torch.randn(4, 3)\n",
    "\n",
    "# QR decomposition\n",
    "Q, R = torch.linalg.qr(A)\n",
    "print(\"QR Decomposition:\")\n",
    "print(f\"Q shape: {Q.shape}, R shape: {R.shape}\")\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "U, S, Vh = torch.linalg.svd(A)\n",
    "print(\"\\nSVD:\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"S shape: {S.shape}\")\n",
    "print(f\"Vh shape: {Vh.shape}\")\n",
    "print(\"Singular values:\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced linear algebra operations\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]])\n",
    "\n",
    "# Matrix determinant\n",
    "det = torch.det(A)\n",
    "print(\"Determinant:\", det)\n",
    "\n",
    "# Matrix inverse\n",
    "inv = torch.inverse(A)\n",
    "print(\"\\nInverse matrix:\")\n",
    "print(inv)\n",
    "\n",
    "# Verify: A @ A_inv = I\n",
    "identity = A @ inv\n",
    "print(\"\\nA @ A_inv (should be identity):\")\n",
    "print(identity)\n",
    "\n",
    "# Matrix norm\n",
    "norm = torch.norm(A)\n",
    "print(\"\\nFrobenius norm:\", norm)\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector operations\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Dot product\n",
    "dot_product = torch.dot(v1, v2)\n",
    "print(\"Dot product:\", dot_product)\n",
    "\n",
    "# Outer product\n",
    "outer = torch.outer(v1, v2)\n",
    "print(\"\\nOuter product:\")\n",
    "print(outer)\n",
    "\n",
    "# Cross product (for 3D vectors)\n",
    "cross = torch.cross(v1, v2)\n",
    "print(\"\\nCross product:\", cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch matrix multiplication\n",
    "# Useful for neural networks with batches\n",
    "batch1 = torch.randn(10, 3, 4)  # 10 matrices of size 3x4\n",
    "batch2 = torch.randn(10, 4, 5)  # 10 matrices of size 4x5\n",
    "\n",
    "result = torch.bmm(batch1, batch2)  # Batch matrix multiply\n",
    "print(\"Batch matrix multiplication:\")\n",
    "print(f\"Input shapes: {batch1.shape} x {batch2.shape}\")\n",
    "print(f\"Output shape: {result.shape}\")  # (10, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "B = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "# Matrix multiplication (@ operator or torch.matmul)\n",
    "result1 = A @ B\n",
    "result2 = torch.matmul(A, B)\n",
    "print(\"\\nA @ B:\")\n",
    "print(result1)\n",
    "\n",
    "# Element-wise multiplication (different from matrix multiplication!)\n",
    "element_wise = A * B\n",
    "print(\"\\nA * B (element-wise):\")\n",
    "print(element_wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrix Operations\n",
    "\n",
    "**Purpose**: Perform linear algebra operations essential for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-order gradients\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# First derivative\n",
    "y = x ** 3\n",
    "y.backward(create_graph=True)  # create_graph=True to compute higher-order derivatives\n",
    "first_grad = x.grad.clone()\n",
    "print(\"First derivative (3x^2):\", first_grad.item())\n",
    "\n",
    "# Second derivative\n",
    "x.grad.zero_()\n",
    "first_grad.backward()\n",
    "second_grad = x.grad\n",
    "print(\"Second derivative (6x):\", second_grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing gradients for non-scalar outputs\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(\"y:\", y)\n",
    "\n",
    "# For non-scalar outputs, need to provide gradient argument\n",
    "# This represents the gradient of some scalar loss w.r.t. y\n",
    "grad_output = torch.tensor([1.0, 1.0, 1.0])\n",
    "y.backward(gradient=grad_output)\n",
    "\n",
    "print(\"Gradient:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Linear regression gradient\n",
    "# y = wx + b\n",
    "\n",
    "# Initialize parameters\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Input data\n",
    "x = torch.tensor([3.0])\n",
    "target = torch.tensor([10.0])\n",
    "\n",
    "# Forward pass\n",
    "prediction = w * x + b\n",
    "print(\"Prediction:\", prediction.item())\n",
    "\n",
    "# Loss (Mean Squared Error)\n",
    "loss = (prediction - target) ** 2\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Gradients\n",
    "print(\"\\nGradient w.r.t. w:\", w.grad)\n",
    "print(\"Gradient w.r.t. b:\", b.grad)\n",
    "\n",
    "# Manual gradient descent step\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad\n",
    "    \n",
    "print(\"\\nUpdated w:\", w.item())\n",
    "print(\"Updated b:\", b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaching from computation graph\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# With gradient tracking\n",
    "y = x ** 2\n",
    "print(\"y requires_grad:\", y.requires_grad)\n",
    "\n",
    "# Detach from graph (no gradient tracking)\n",
    "y_detached = y.detach()\n",
    "print(\"y_detached requires_grad:\", y_detached.requires_grad)\n",
    "\n",
    "# Using torch.no_grad() context\n",
    "with torch.no_grad():\n",
    "    z = x ** 3\n",
    "    print(\"z (inside no_grad) requires_grad:\", z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation and zeroing\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(\"First gradient:\", x.grad)\n",
    "\n",
    "# Second computation (gradients accumulate!)\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(\"After second backward (accumulated):\", x.grad)\n",
    "\n",
    "# Zero gradients before next computation\n",
    "x.grad.zero_()\n",
    "y3 = x ** 2\n",
    "y3.backward()\n",
    "print(\"After zeroing and new backward:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "\n",
    "# Define computation: y = sum(3x^2 + 2x + 1)\n",
    "y = torch.sum(3 * x**2 + 2 * x + 1)\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Gradient: dy/dx = 6x + 2\n",
    "print(\"Gradient (6x + 2):\", x.grad)\n",
    "print(\"Expected: [8, 14, 20]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic gradient computation\n",
    "# Create tensor with requires_grad=True to track operations\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "print(\"requires_grad:\", x.requires_grad)\n",
    "\n",
    "# Define a function: y = x^2\n",
    "y = x ** 2\n",
    "print(\"\\ny = x^2:\", y)\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "y.backward()  # Compute gradients\n",
    "\n",
    "# Gradient is stored in x.grad\n",
    "print(\"dy/dx:\", x.grad)  # Should be 2*x = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Autograd and Gradients\n",
    "\n",
    "**Purpose**: Automatic differentiation for backpropagation in neural networks.\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation engine that powers neural network training. It tracks operations on tensors and computes gradients automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the main PyTorch functionalities:\n",
    "\n",
    "1. **Tensor Creation**: Various methods to initialize tensors\n",
    "2. **Tensor Properties**: Understanding shape, dtype, device, etc.\n",
    "3. **Tensor Operations**: Arithmetic and mathematical operations\n",
    "4. **Indexing and Slicing**: Accessing and modifying tensor elements\n",
    "5. **Reshaping**: Manipulating tensor dimensions\n",
    "6. **Matrix Operations**: Linear algebra for deep learning\n",
    "7. **Autograd**: Automatic differentiation for training neural networks\n",
    "8. **GPU Operations**: Accelerating computations with CUDA\n",
    "\n",
    "These fundamentals form the foundation for building and training neural networks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU operations (if multiple GPUs available)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Found {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # Create tensors on different GPUs\n",
    "    x_gpu0 = torch.randn(3, 3, device='cuda:0')\n",
    "    x_gpu1 = torch.randn(3, 3, device='cuda:1')\n",
    "    \n",
    "    print(\"Tensor on GPU 0:\", x_gpu0.device)\n",
    "    print(\"Tensor on GPU 1:\", x_gpu1.device)\n",
    "    \n",
    "    # Move between GPUs\n",
    "    x_gpu1_copy = x_gpu0.to('cuda:1')\n",
    "    print(\"Copied to GPU 1:\", x_gpu1_copy.device)\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    print(\"Single GPU system\")\n",
    "else:\n",
    "    print(\"No GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management on GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU memory allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "    print(\"GPU memory cached:\", torch.cuda.memory_reserved() / 1024**2, \"MB\")\n",
    "    \n",
    "    # Create large tensor\n",
    "    large_tensor = torch.randn(1000, 1000, device='cuda')\n",
    "    print(\"\\nAfter creating large tensor:\")\n",
    "    print(\"GPU memory allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "    \n",
    "    # Free memory\n",
    "    del large_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nAfter freeing and clearing cache:\")\n",
    "    print(\"GPU memory allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "else:\n",
    "    print(\"GPU memory management not available (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: CPU vs GPU (if available)\n",
    "import time\n",
    "\n",
    "size = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CPU computation\n",
    "x_cpu = torch.randn(size, size)\n",
    "y_cpu = torch.randn(size, size)\n",
    "\n",
    "start = time.time()\n",
    "z_cpu = torch.matmul(x_cpu, y_cpu)\n",
    "cpu_time = time.time() - start\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# GPU computation (if available)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.to(device)\n",
    "    y_gpu = y_cpu.to(device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    _ = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .to() method (more flexible)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original device:\", x.device)\n",
    "\n",
    "# Move to target device\n",
    "x = x.to(device)\n",
    "print(\"After .to(device):\", x.device)\n",
    "\n",
    "# Specify dtype and device together\n",
    "y = torch.randn(2, 3).to(device=device, dtype=torch.float64)\n",
    "print(\"\\ny with specific dtype and device:\")\n",
    "print(\"Device:\", y.device)\n",
    "print(\"Dtype:\", y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device-agnostic code (best practice)\n",
    "# This pattern works whether CUDA is available or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create tensors directly on the device\n",
    "x = torch.randn(3, 3, device=device)\n",
    "y = torch.ones(3, 3, device=device)\n",
    "\n",
    "print(\"\\nx:\")\n",
    "print(x)\n",
    "print(\"Device:\", x.device)\n",
    "\n",
    "# Operations on GPU tensors\n",
    "z = x + y\n",
    "print(\"\\nz = x + y:\")\n",
    "print(z)\n",
    "print(\"Device:\", z.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors between CPU and GPU\n",
    "x_cpu = torch.tensor([1, 2, 3])\n",
    "print(\"CPU tensor:\", x_cpu)\n",
    "print(\"Device:\", x_cpu.device)\n",
    "\n",
    "# Move to GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.cuda()  # or x_cpu.to('cuda')\n",
    "    print(\"\\nGPU tensor:\", x_gpu)\n",
    "    print(\"Device:\", x_gpu.device)\n",
    "    \n",
    "    # Move back to CPU\n",
    "    x_back = x_gpu.cpu()  # or x_gpu.to('cpu')\n",
    "    print(\"\\nBack to CPU:\", x_back)\n",
    "    print(\"Device:\", x_back.device)\n",
    "else:\n",
    "    print(\"\\nSkipping GPU transfer (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"\\nNote: CUDA is not available on this system.\")\n",
    "    print(\"GPU examples will use CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GPU Operations (MPS)\n",
    "\n",
    "**Purpose**: Accelerate computations by utilizing GPU hardware.\n",
    "\n",
    "PyTorch supports MPS-enabled GPUs for massive performance improvements, especially for large-scale deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GPU Operations (CUDA)\n",
    "\n",
    "**Purpose**: Accelerate computations by utilizing GPU hardware.\n",
    "\n",
    "PyTorch supports CUDA-enabled GPUs for massive performance improvements, especially for large-scale deep learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
